[{"title":"Redshift优化策略","url":"/btjs.github.io/2019/11/26/Redshift%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5/","content":"<h5 id=\"Redshift\"><a href=\"#Redshift\" class=\"headerlink\" title=\"Redshift\"></a>Redshift</h5><ul>\n<li><p>待看 ： <a href=\"https://amazonaws-china.com/cn/blogs/china/amazon-redshift-table-design-databasedata/\" target=\"_blank\" rel=\"noopener\">https://amazonaws-china.com/cn/blogs/china/amazon-redshift-table-design-databasedata/</a></p>\n</li>\n<li><p>数据块</p>\n<ul>\n<li>select col, max(blocknum) from stv_blocklist b, stv_tbl_perm p where (b.tbl=p.id) and name =’（tablename）’ and col &lt; 17 group by name, col order by col;</li>\n</ul>\n</li>\n<li><p>建议优化的压缩方式</p>\n<ul>\n<li>analyze compression (tablename);</li>\n</ul>\n</li>\n<li><p>查看当前表的压缩格式</p>\n<ul>\n<li>select tablename,”column”, type,distkey, sortkey,encoding  from pg_table_def where tablename like ‘(tablename))’; </li>\n</ul>\n</li>\n<li><p>记录redshift正在运行的pid</p>\n<ul>\n<li>select pid, starttime, duration,trim(user_name) as user,trim (query) as querytxt from stv_recents where status = ‘Running’;</li>\n<li>取消- cacel 18063</li>\n</ul>\n</li>\n<li><p>查看分配方式的分片方式</p>\n<ul>\n<li>select trim(name) as table, slice, sum(num_values) as rows, min(minvalue), max(maxvalue) from svv_diskusage where name in (‘tablename’) and col =0 group by name, slice order by name, slice;</li>\n</ul>\n</li>\n<li><p>找出表约束</p>\n<ul>\n<li>select constraint_name, constraint_type from information_schema.table_constraints where constraint_schema =’public’ and table_name = ‘tablename’;</li>\n</ul>\n</li>\n<li><p>删除表的约束</p>\n<ul>\n<li>alter table tb_archerygo drop constraint “key”;</li>\n</ul>\n</li>\n<li><p>查看分配方式 0 EVEN 1    KEY 8    ALL 10 AUTO (ALL) 11 AUTO (EVEN) </p>\n<ul>\n<li>select relname,RELEFFECTIVEDISTSTYLE from PG_CLASS_INFO where relname = ‘tablename’ limit 10;</li>\n</ul>\n</li>\n<li><p>缓存设置，自测时应关闭 off</p>\n<ul>\n<li>show enable_result_cache_for_session;</li>\n</ul>\n</li>\n<li><p>查看表的排序键</p>\n<ul>\n<li>select “table”,diststyle,sortkey1,sortkey_num,”size”,unsorted,tbl_rows,skew_sortkey1,skew_rows from  SVV_TABLE_INFO WHERE “table” = ‘tablename’;</li>\n</ul>\n</li>\n<li><p>分析表  has_dist_key：指示该表有没有分配键。1 有；0 没有 has_sort_key：指示该表有没有排序键 has_column_encoding：是否已为任意列定义任何压缩编码 </p>\n</li>\n<li><p>ratio_skew_across_slices  数据分配偏斜的指示。值越小越好。   pct_slices_populated ：切片的填充百分比。值越大越好。</p>\n<ul>\n<li>SELECT SCHEMA schemaname,”table” tablename,table_id tableid,size size_in_mb,CASE WHEN diststyle NOT IN (‘EVEN’,’ALL’) THEN 1 ELSE 0 END has_dist_key,CASE WHEN sortkey1 IS NOT NULL THEN 1 ELSE 0 END has_sort_key,CASE WHEN encoded = ‘Y’ THEN 1 ELSE 0 END has_col_encoding,CAST(max_blocks_per_slice - min_blocks_per_slice AS FLOAT) / GREATEST(NVL (min_blocks_per_slice,0)::int,1) ratio_skew_across_slices,CAST(100<em>dist_slice AS FLOAT) /(SELECT COUNT(DISTINCT slice) FROM stv_slices) pct_slices_populated FROM svv_table_info ti JOIN (SELECT tbl,MIN(c) min_blocks_per_slice,MAX(c) max_blocks_per_slice,COUNT(DISTINCT slice) dist_slice FROM (SELECT b.tbl,b.slice,COUNT(</em>) AS c FROM STV_BLOCKLIST b GROUP BY b.tbl,b.slice) WHERE tbl IN (SELECT table_id FROM svv_table_info) GROUP BY tbl) iq ON iq.tbl = ti.table_id WHERE “table” = ‘tablename’;</li>\n</ul>\n</li>\n<li><p>创建交错排序键</p>\n<ul>\n<li>create table cust_sales_date_interleaved interleaved sortkey (c_custkey, c_region, c_mktsegment, d_date) as select * from cust_sales_date; 或 create table() DISTSTYLE even |[DISTSTYLE KEY DISTKEY(column_name)] INTERLEAVED SORTKEY (logtime, event,platform,version,userlabel,gamemode, sub14, sub15);</li>\n</ul>\n</li>\n<li><p>使用查询计划观察分配情况，只有DS_DIST_NONE 和 DS_DIST_ALL_NONE 效果最佳 （分配方式通常作用在连接中）</p>\n</li>\n</ul>\n","categories":["数据仓库"],"tags":["redshift"]},{"title":"Shell小记","url":"/btjs.github.io/2019/11/26/Shell%E5%B0%8F%E8%AE%B0/","content":"<h6 id=\"符号-后的括号\"><a href=\"#符号-后的括号\" class=\"headerlink\" title=\"符号$后的括号\"></a>符号$后的括号</h6><ol>\n<li>${a} 变量a的值, 在不引起歧义的情况下可以省略大括号。</li>\n<li>$(cmd) 命令替换，和<code>cmd</code>效果相同，结果为shell命令cmd的输，不过某些Shell版本不支持$()形式的命令替换, 如tcsh。</li>\n<li>$((expression)) 和<code>exprexpression</code>效果相同, 计算数学表达式expr的数值, 其中expr只要符合C语言的运算规则即可, 甚至三目运算符和逻辑表达式都可以计算。</li>\n</ol>\n<ul>\n<li><p>技巧小结：</p>\n<ul>\n<li>字符串比较用双中括号[[ ]]；算数比较用单中括号[ ]——左右留空格</li>\n<li>算数运算用双小括号(( )) ；shell命令及输出用小括号( )——左右不留空格</li>\n<li>快速替换用花括号{ }——左右留空格</li>\n<li>反单引号起着命令替换的作用<code></code></li>\n<li>单括号()：$(command), 等同于a=$<code>command</code>,得到命令输出传递给变量a；小括号中的内容会开启一个子shell独立运行；括号中以分号连接，最后一个命令不需要；各命令和括号无空格</li>\n<li>双括号(())：省去$符号的算术运算—— for((i=0;i&lt;5;i++))； if (($i&lt;5))； a=5; ((a++)) 可将 $a 重定义为6；括号内支持多个表达式用逗号分开。</li>\n<li>单中括号[ ]：字符串比较——==和!=   整数比较—— -ne:不等于：-gt：大于；-lt ：小于；-eq：等于；   数组索引——array[0]</li>\n<li>双中括号[[]]：<ol>\n<li>字符串比较——可以把右边的作为一个模式，而不仅仅是一个字符串，比如[[ hello == hell? ]]，结果为真。[[ ]] 中匹配字符串或通配符，不需要引号。</li>\n<li>逻辑运算符——防止脚本许多逻辑错误，比如，&amp;&amp;、||、&lt;和&gt; 操作符能够正常存在于[[ ]]条件判断结构中</li>\n<li>退出码——bash把双中括号中的表达式看作一个单独的元素，并返回一个退出状态码。</li>\n</ol>\n</li>\n</ul>\n<pre><code>示例：\n  if ($i&lt;5)    \n  if [ $i -lt 5 ]    \n  if [ $a -ne 1 -a $a != 2 ]    \n  if [ $a -ne 1] &amp;&amp; [ $a != 2 ]    \n  if [[ $a != 1 &amp;&amp; $a != 2 ]]    \n\n  for i in $(seq 0 4);do echo $i;done    \n  for i in `seq 0 4`;do echo $i;done    \n  for ((i=0;i&lt;5;i++));do echo $i;done    \n  for i in {0..4};do echo $i;done </code></pre></li>\n</ul>\n<h6 id=\"替换\"><a href=\"#替换\" class=\"headerlink\" title=\"替换\"></a>替换</h6><ul>\n<li>特殊替换——${var:-string},${var:+string},${var:=string},${var:?string}</li>\n<li>模式匹配替换——${variable%pattern}，${variable%%pattern}，${variable#pattern}，${variable##pattern} “# 是去掉左边(在键盘上#在$之左边)；% 是去掉右边(在键盘上%在$之右边)；#和%中的单一符号是最小匹配，两个相同符号是最大匹配。”</li>\n</ul>\n","categories":["Linux"],"tags":["shell"]},{"title":"笨笨学算法(一)","url":"/btjs.github.io/2019/11/26/%E7%AC%A8%E7%AC%A8%E5%AD%A6%E7%AE%97%E6%B3%95-%E4%B8%80/","content":"<h5 id=\"二分查找法（折半查找法）\"><a href=\"#二分查找法（折半查找法）\" class=\"headerlink\" title=\"二分查找法（折半查找法）\"></a>二分查找法（折半查找法）</h5><p>~<br>//二分查找法（折半查找法）<br>    //前提： 在一个有序集合中，不重复，依赖数组(链表大O会高)，数据量不要太小，也不要太大(数组讲究连续性)<br>    //注意：<br>    //  1.折半方法建议：low + (high - low) / 2<br>    // ‘× [(low+high)/2] 减少数值溢出可能’ ‘× low + (high-low) &gt;&gt; 1 :注意运算符优先级’<br>    //  2.考虑循环边界问题 low &lt;= high<br>    //  3.注意加1减1，否则死循环<br>    //时间复杂度 O(logn):<br>    //   区间变化 -&gt; n,n/2,n/4,n/8,….,n/2^k; k代表需要经历的次数，所以用O(k), n/2^k = 1 -&gt; k = log2n<br>    //讨论： O(1)一定比O(logn)高效吗？<br>    //  2的32次方用二分查找对多32次，O(1) 通常来说会去掉常数，系数和低阶，可能表示非常大的数O(1000),O(100000)<br>    //场景：假设有1000万个整数数据，每个数据占8个字节，如何设计数据结构和算法，快速判断某个整数是否出现在这1000万数据中？<br>    //要求不要超过100MB.<br>    //   1000万数据放在数组中大约占80MB,先排序，在二分查找. (散列表，二叉树需要额外存储空间)<br>    public int bsearch(int[] a, int n, int sValue) {<br>        if(a == null || a.length &lt;= 0) return -1;<br>        int low = 0;<br>        int high = n - 1;<br>        while (low &lt;= high) {  //带上=<br>            int mid = low + (high - low) / 2;<br>            if(a[mid] == sValue) {<br>                return mid;<br>            }else if(a[mid] &gt; sValue) {<br>                high = mid - 1; //需要减1 ：注意死循环<br>            }else {<br>                low = mid + 1; //同上<br>            }<br>        }<br>        return -1;<br>    }</p>\n<pre><code>//练习：求一个数的平方根？要求精确到小数点后6位\npublic int squre(int[] a, int n) {\n    if(a == null || a.length &lt;= 0) throw new IllegalArgumentException(&quot;非法参数&quot;);\n\n    return 0;\n}</code></pre><p>~</p>\n","categories":["数据结构与算法"],"tags":["数据结构与算法"]},{"title":"Elasticsearch学习","url":"/btjs.github.io/2019/11/25/Elasticsearch%E5%AD%A6%E4%B9%A0/","content":"<h6 id=\"Elasticsearch\"><a href=\"#Elasticsearch\" class=\"headerlink\" title=\"Elasticsearch\"></a>Elasticsearch</h6><ul>\n<li>这是一个全文信息检索工具也被称为搜索引擎</li>\n</ul>\n<p>####### 概念</p>\n<ul>\n<li>请先理解Lucene</li>\n</ul>\n","categories":["搜索引擎"],"tags":["搜索"]},{"title":"dubbo学习","url":"/btjs.github.io/2019/11/25/dubbo%E5%AD%A6%E4%B9%A0/","content":"","categories":["Dubbo系列"],"tags":["分布式"]},{"title":"Lucene学习","url":"/btjs.github.io/2019/11/24/Lucene%E5%AD%A6%E4%B9%A0/","content":"<h5 id=\"Lucene\"><a href=\"#Lucene\" class=\"headerlink\" title=\"Lucene\"></a>Lucene</h5><ul>\n<li>为应用程序提供索引和搜索功能的开源项目</li>\n</ul>\n","categories":["搜索引擎"],"tags":["搜索"]},{"title":"Spring篇之数据源","url":"/btjs.github.io/2019/11/17/Spring%E7%AF%87%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%BA%90/","content":"","categories":["Spring系列"],"tags":["Spring系列"]},{"title":"java集合小记","url":"/btjs.github.io/2019/10/26/java%E9%9B%86%E5%90%88%E5%B0%8F%E8%AE%B0/","content":"","categories":["大数据"],"tags":["java"]},{"title":"hadoop小记","url":"/btjs.github.io/2019/10/26/hadoop%E5%AD%A6%E4%B9%A0%E5%88%9D%E7%AF%87-%E4%B8%80/","content":"<h2 id=\"hadoop学习小记-一\"><a href=\"#hadoop学习小记-一\" class=\"headerlink\" title=\"hadoop学习小记(一)\"></a>hadoop学习小记(一)</h2><p> <strong><em>MapReduce</em></strong>:<br> 一种分布式的计算方式指定一个Map（映#x5C04;）函数，用来把一组键值对映射成一组新的键值对，指定并发的Reduce（归约）函数，用来保证所有映射的键值对中的每一个共享相同的键组</p>\n<p> map: (K1, V1) → list(K2, V2) combine: (K2, list(V2)) → list(K2, V2) reduce: (K2, list(V2)) → list(K3, V3)</p>\n<p>Map输出格式和Reduce输入格式一定是相同的</p>\n<p><strong><em>MapReduce - 读取数据</em></strong><br>通过InputFormat决定读取的数据的类型，然后拆分成一个个InputSplit，每个InputSplit对应一个Map处理，RecordReader读取InputSplit的内容给Map</p>\n","categories":["大数据"],"tags":["hadoop"]},{"title":"Kafka学习","url":"/btjs.github.io/2019/09/10/Kafka%E5%AD%A6%E4%B9%A0/","content":"","categories":["大数据"],"tags":["kafka"]},{"title":"Spark学习(Spark Streaming)","url":"/btjs.github.io/2019/09/04/Spark%E5%AD%A6%E4%B9%A0-Spark-Streaming/","content":"","categories":["大数据"],"tags":["spark"]},{"title":"Spark学习(Spark SQL)","url":"/btjs.github.io/2019/09/03/Spark%E5%AD%A6%E4%B9%A0-Spark-SQL/","content":"<h6 id=\"DataFrame-Spark-SQL\"><a href=\"#DataFrame-Spark-SQL\" class=\"headerlink\" title=\"DataFrame(Spark SQL)\"></a>DataFrame(Spark SQL)</h6><ul>\n<li><p>Spark使用全新的SparkSession接口替代Spark1.6中的SQLContext及HiveContext接口来实现其对数据加载、转换、处理等功能。SparkSession实现了SQLContext及HiveContext所有功能。</p>\n</li>\n<li><p>SparkSession支持从不同的数据源加载数据，并把数据转换成DataFrame，并且支持把DataFrame转换成SQLContext自身中的表，然后使用SQL语句来操作数据。<br>SparkSession亦提供了HiveQL以及其他依赖于Hive的功能的支持。</p>\n</li>\n<li><p>创建:<code>scala&gt; val spark=SparkSession.builder().getOrCreate()</code></p>\n</li>\n<li><p>从RDD转换得到DataFrame:</p>\n<ol>\n<li>利用反射来推断包含特定类型对象的RDD的schema，适用对已知数据结构的RDD转换；</li>\n<li>使用编程接口，构造一个schema并将其应用在已知的RDD上。</li>\n</ol>\n</li>\n<li><p>把RDD保存成文件</p>\n<ul>\n<li><code>val peopleDF = spark.read.format(&quot;json&quot;).load(&quot;file:///usr/local/spark/examples/src/main/resources/people.json&quot;)</code></li>\n</ul>\n<ol>\n<li><code>peopleDF.select(&quot;name&quot;, &quot;age&quot;).write.format(&quot;csv&quot;).save(&quot;file:///usr/local/spark/mycode/newpeople.csv&quot;)</code></li>\n<li><code>df.rdd.saveAsTextFile(&quot;file:///usr/local/spark/mycode/newpeople.txt&quot;)</code></li>\n</ol>\n</li>\n</ul>\n","categories":["大数据"],"tags":["spark"]},{"title":"Spark学习(初识RDD)","url":"/btjs.github.io/2019/08/29/Spark%E5%AD%A6%E4%B9%A0-%E5%88%9D%E8%AF%86-RDD/","content":"<h6 id=\"基本概念\"><a href=\"#基本概念\" class=\"headerlink\" title=\"基本概念\"></a>基本概念</h6><ul>\n<li>RDD: 弹性分布式数据集，是分布式内存的抽象，提供一种高度受限的共享内存模型</li>\n<li>DAG: 是有向无环图的简称，反映RDD间的依赖关系</li>\n<li>Executor：是运行在工作节点上的一个进程，负责运行任务，并为应用程序存储数据</li>\n<li>应用：用户编写的Spark程序</li>\n<li>任务：运行在Executor上的工作单元</li>\n<li>作业：一个作业包含多个RDD及作用与相应RDD上的各种操作</li>\n<li>阶段：是作业的基本调度单位，一个作业会分为多组任务，每组任务被称为“阶段”，或“任务集”</li>\n</ul>\n<h6 id=\"架构设计\"><a href=\"#架构设计\" class=\"headerlink\" title=\"架构设计\"></a>架构设计</h6><ul>\n<li><p><em>集群资源管理器(Cluster Manager)</em> 、 <em>运行作业任务的工作节点(Worker Node)</em> 、 <em>每个应用的任务控制节点(Driver)</em> 、 <em>每个工作节点上负责具体任务的执行进程(Executor)</em></p>\n</li>\n<li><p>与mr对比，spark采用的Executor的优点:</p>\n<ol>\n<li>利用多线程来完成具体任务(mr是进程模型)</li>\n<li>Executor中有一个BlockManager存储模型，会将内存和磁盘共同作为存储设备，当需要迭代计算时，可以将中间结果存储到这个模型块里，下次需要时，可以直接读取该模型里的数据，<br>减少io开销；或在交互式查询的场景下，预先将表缓存到该存储系统上，从而可以提高读写IO性能。</li>\n</ol>\n</li>\n<li><p><strong>在Spark中，一个应用(Application)由一个任务控制节点(Driver)和若干个作业(job)构成，一个作业由多个阶段(Stage)构成，一个阶段由多个任务(Task)组成。</strong><br><em>当执行一个应用时，任务控制节点会向集群管理器申请资源，启动Executor，并向Executor发送应用程序代码和文件，然后在Executor上执行任务，运行结束后，执行结果会返回给控制节点，<br>或者写入HDFS或者其他数据库中</em></p>\n</li>\n</ul>\n<h6 id=\"一个流程的讲解\"><a href=\"#一个流程的讲解\" class=\"headerlink\" title=\"一个流程的讲解\"></a>一个流程的讲解</h6><ul>\n<li><p>Spark运行基本流程</p>\n<ol>\n<li><strong>当一个spark应用被提交时，首先需要为这个应用构建起基本的运行环境，即由任务控制节点(Driver)创建一个SparkContext，由SparkContext负责和资源管理器(Cluster Manager)<br>的通信以及进行资源的申请、任务的分配和监控等。SparkContext会向资源管理器注册并申请运行Executor的资源</strong> </li>\n<li><strong>资源管理器为Executor分配资源，并启动Executor进程，Executor运行情况将随着“心跳”发送到资源管理器上</strong></li>\n<li><strong>SparkContext根据RDD的依赖关系构建DAG图，DAG图提交给DAG调度器(DAGSheduler)进行解析，将DAG图分解成多个“阶段”(每个阶段都是一个任务集)，<br>并且计算出各个阶段之间的依赖关系，然后把一个个“任务集”提交给底层的任务调度器(TaskScheduler)进行处理；Executor向SparkContext申请任务，任务调度器将任务分发给Executor运行，<br>同时，SparkContext将应用程序代码发放给Executor</strong></li>\n<li><strong>任务在Executor上运行，把执行结果反馈给任务调度器，然后反馈给DAG调度器，运行完毕后写入数据并释放所有资源。</strong> </li>\n</ol>\n</li>\n<li><p>Spark运行架构具有以下特点：</p>\n<ol>\n<li>每个应用都有自己专属的Executor进程，并且该进程在应用运行期间一直驻留。Executor进程以多线程的方式运行任务，减少了多进程任务频繁的启动开销，使得任务执行变得非常高效和可靠；</li>\n<li>Spark运行过程与资源管理器无关，只要能够获取Executor进程并保持通信即可；</li>\n<li>Executor上有一个BlockManager存储模块，类似于键值存储系统（把内存和磁盘共同作为存储设备），在处理迭代计算任务时，不需要把中间结果写入到HDFS等文件系统，<br>而是直接放在这个存储系统上，后续有需要时就可以直接读取；在交互式查询场景下，也可以把表提前缓存到这个存储系统上，提高读写IO性能；</li>\n<li>任务采用了数据本地性和推测执行等优化机制。数据本地性是尽量将计算移到数据所在的节点上进行，即“计算向数据靠拢”，因为移动计算比移动数据所占的网络资源要少得多。<br>而且，Spark采用了延时调度机制，可以在更大的程度上实现执行过程优化。</li>\n</ol>\n</li>\n<li><p>思考： 拥有数据的节点当前正被其他的任务占用，那么，在这种情况下是否需要将数据移动到其他的空闲节点呢？</p>\n<ul>\n<li><strong>不一定。因为，如果经过预测发现当前节点结束当前任务的时间要比移动数据的时间还要少，那么，调度就会等待，直到当前节点可用。</strong></li>\n</ul>\n</li>\n</ul>\n<h6 id=\"RDD基础\"><a href=\"#RDD基础\" class=\"headerlink\" title=\"RDD基础\"></a>RDD基础</h6><ul>\n<li><p>一个RDD就是一个分布式对象集合，本质上是一个只读的分区记录集合，每个RDD可以分成多个分区，每个分区就是一个数据集片段，并且一个RDD的不同分区可以被保存到集群中不同的节点上，<br>从而可以在集群中的不同节点上进行并行计算。</p>\n</li>\n<li><p>RDD是只读的分区集合，不能直接修改，只能基于稳定的物理存储中的数据集来创建RDD，或者通过在其他RDD执行确定的转换操作(map、join、groupby)得到;</p>\n</li>\n</ul>\n<h6 id=\"RDD编程\"><a href=\"#RDD编程\" class=\"headerlink\" title=\"RDD编程\"></a>RDD编程</h6><ul>\n<li><p>RDD创建：</p>\n<ol>\n<li>读取一个外部数据集;(集群方式不能使用本地文件file:形式)  <code>sc.textFile</code><br>从本地文件加载数据集，或者从HDFS文件系统、HBase、Cassandra、Amazon S3等外部数据源中加载数据集。<br>Spark可以支持文本文件、SequenceFile文件（Hadoop提供的 SequenceFile是一个由二进制序列化过的key/value的字节流组成的文本存储文件）<br>和其他符合Hadoop InputFormat格式的文件。</li>\n<li>调用SparkContext的parallelize方法，在Driver中一个已经存在的集合（数组）上创建。 <code>sc.parallelize(array)</code></li>\n</ol>\n</li>\n<li><p>RDD操作：</p>\n<ol>\n<li>转换(Transformation) ： 基于现有的数据集创建一个新的数据集。转换是惰性的，只有行动会触发真正的计算<ul>\n<li>filter(func)：筛选出满足函数func的元素，并返回一个新的数据集</li>\n<li>map(func)：将每个元素传递到函数func中，并将结果返回为一个新的数据集</li>\n<li>flatMap(func)：与map()相似，但每个输入元素都可以映射到0或多个输出结果</li>\n<li>groupByKey()：应用于(K,V)键值对的数据集时，返回一个新的(K, Iterable)形式的数据集</li>\n<li>reduceByKey(func)：应用于(K,V)键值对的数据集时，返回一个新的(K, V)形式的数据集，其中的每个值是将每个key传递到函数func中进行聚合</li>\n</ul>\n</li>\n<li>行动（Action）：在数据集上进行运算，返回计算值。<ul>\n<li>count() 返回数据集中的元素个数</li>\n<li>collect() 以数组的形式返回数据集中的所有元素</li>\n<li>first() 返回数据集中的第一个元素</li>\n<li>take(n) 以数组的形式返回数据集中的前n个元素</li>\n<li>reduce(func) 通过函数func（输入两个参数并返回一个值）聚合数据集中的元素</li>\n<li>foreach(func) 将数据集中的每个元素传递到函数func中运行</li>\n</ul>\n</li>\n</ol>\n</li>\n<li><p>RDD持久化：缓存中间数据，省去从头计算的过程，在第一次计算的时候才被缓存。使用cache()方法时，会调用persist(MEMORY_ONLY)。<br>persist()的圆括号中包含的是持久化级别参数，比如，persist(MEMORY_ONLY)表示将RDD作为反序列化的对象存储于JVM中，如果内存不足，就要按照LRU原则替换缓存中的内容。<br>persist(MEMORY_AND_DISK)表示将RDD作为反序列化的对象存储在JVM中，如果内存不足，超出的分区将会被存放在硬盘上。最后，可以使用unpersist()方法手动地把持久化的RDD从缓存中移除。</p>\n</li>\n<li><p>RDD分区：RDD是弹性分布式数据集，通常RDD很大，会被分成很多分区，分别保存在不同的节点上。RDD分区原则是尽量保持与集群中的CPU核心数；一般：</p>\n<ol>\n<li>本地模式：默认为本地机器的CPU数目，若设置了local[N],则默认为N；</li>\n<li>Apache Mesos：默认的分区数为8；</li>\n<li>Standalone或YARN：在“集群中所有CPU核心数目总和”和“2”二者中取较大值作为默认值；</li>\n</ol>\n</li>\n<li><p>对于parallelize而言，如果没有在方法中指定分区数，则默认为spark.default.parallelism</p>\n</li>\n<li><p>对于textFile而言，如果没有在方法中指定分区数，则默认为min(defaultParallelism,2)，其中，defaultParallelism对应的就是spark.default.parallelism；</p>\n</li>\n<li><p>如果是从HDFS中读取文件，则分区数为文件分片数(比如，128MB/片)。</p>\n</li>\n<li><p>RDD的打印方式：</p>\n<ul>\n<li>当采用本地模式（local）在单机上执行时 -&gt; <code>rdd.foreach(println)或者rdd.map(println)</code></li>\n<li>当采用集群模式执行时,在worker节点上执行打印语句是输出到worker节点的stdout上，而不是输出到任务控制节点Driver Program中。可以使用collect()方法，例如<br><code>rdd.collect().foreach(println)</code>，不过此方法会拉取所有worker节点的rdd元素到Driver Program上，可能导致OOM，故可以采用<code>rdd.take(100).foreach(println)</code>方式</li>\n</ul>\n</li>\n</ul>\n<h6 id=\"键值对RDD\"><a href=\"#键值对RDD\" class=\"headerlink\" title=\"键值对RDD\"></a>键值对RDD</h6><ul>\n<li><p>RDD可以包括任意类型，但是“键值对”是一种比较常见的RDD元素类型应用于map/reduce操作。普通RDD里面存储的数据类型是Int、String等。</p>\n</li>\n<li><p>RDD键值对的创建：</p>\n<ul>\n<li>第一种：从文件中加载<code>val pairRDD = lines.flatMap(line =&gt; line.split(&quot; &quot;)).map(word =&gt; (word,1))</code></li>\n<li>第二种：通过并行集合（数组）创建RDD<code>val pairRDD = rdd.map(word =&gt; (word,1))</code></li>\n</ul>\n</li>\n<li><p>常用的键值对转换操作</p>\n<ol>\n<li>reduceByKey(func)：func对应lambda表达式，根据相同的key聚合相应value</li>\n<li>reduceByKey()：根据相同的key集合于value 例：<code>(Spark,CompactBuffer(1, 1))</code></li>\n<li>keys：将key返回形成一个新的RDD，不会去重</li>\n<li>values：将value返回形成一个新的RDD，不会去重</li>\n<li>sortByKey()：返回一个根据键排序的RDD。</li>\n<li>mapValues(func)：对键值对RDD的value部分进行处理，而不是同时对key和value进行处理</li>\n<li>join：join的类型也和关系数据库中的join一样</li>\n</ol>\n</li>\n</ul>\n<h6 id=\"共享变量\"><a href=\"#共享变量\" class=\"headerlink\" title=\"共享变量\"></a>共享变量</h6><ul>\n<li><p>在默认情况下，当Spark在集群的多个不同节点的多个任务上并行运行一个函数时，它会把函数中涉及到的每个变量，在每个任务上都生成一个副本。<br>但是，有时候，需要在多个任务之间共享变量，或者在任务（Task）和任务控制节点（Driver Program）之间共享变量。为了满足这种需求，Spark提供了两种类型的变量：<br>广播变量（broadcast variables）和累加器（accumulators）。广播变量用来把变量在所有节点的内存之间进行共享。累加器则支持在所有不同节点之间进行累加计算（比如计数或者求和）。</p>\n</li>\n<li><p><strong>广播变量</strong></p>\n<ul>\n<li>当跨越多个阶段的那些任务需要相同的数据，或者当以反序列化方式对数据进行缓存是非常重要的。</li>\n<li>一旦广播变量创建后，普通变量v的值就不能再发生修改，从而确保所有节点都获得这个广播变量的相同的值。</li>\n</ul>\n</li>\n<li><p><strong>累加器</strong></p>\n<ul>\n<li>一个数值型的累加器，可以通过调用SparkContext.longAccumulator()或者SparkContext.doubleAccumulator()来创建。</li>\n<li>运行在集群中的任务，就可以使用add方法来把数值累加到累加器上，但是，这些任务只能做累加操作，不能读取累加器的值，<br>只有任务控制节点（Driver Program）可以使用value方法来读取累加器的值。</li>\n</ul>\n</li>\n</ul>\n","categories":["大数据"],"tags":["spark"]},{"title":"Hive小记","url":"/btjs.github.io/2019/08/29/hive%E5%B0%8F%E8%AE%B0/","content":"","categories":["大数据"],"tags":["hive"]},{"title":"HDFS随笔","url":"/btjs.github.io/2019/08/29/HDFS%E9%9A%8F%E7%AC%94/","content":"<h4 id=\"HDFS的工作机制\"><a href=\"#HDFS的工作机制\" class=\"headerlink\" title=\"HDFS的工作机制\"></a>HDFS的工作机制</h4><ol>\n<li>HDFS集群分为两大角色： namenode和datanode</li>\n<li>NameNode负责管理整个文件系统的元数据（元数据指：文件数据块放置在DataNode位置和数量等信息）</li>\n<li>DataNode负责管理用户的文件数据块</li>\n<li>文件会按照固定的大小（blocksize）切分成若干块后分布式存储在若干台datanode上</li>\n<li>每个文件块可以有多个副本（默认3个），存放在不同的datanode上（一个机架节点上，不同机架的节点上及其他节点上）</li>\n<li>DataNode会定期向NameNode汇报自身所保存的文件block信息，而NameNode则会负责保持文件的副本数量</li>\n<li>HDFS的内部工作机制对客户端保持透明，客户端请求访问HDFS都是通过向NameNode申请来进行的</li>\n</ol>\n<h4 id=\"HDFS写数据流程\"><a href=\"#HDFS写数据流程\" class=\"headerlink\" title=\"HDFS写数据流程\"></a>HDFS写数据流程</h4><h6 id=\"简述\"><a href=\"#简述\" class=\"headerlink\" title=\"简述\"></a>简述</h6><p><em>客户端向HDFS写数据，首先请求NameNode通信以确认可以写文件并获得接收文件block的DataNode，然后，客户端桉<strong>顺序</strong>逐个以一定<br>packet传递给相应DataNode，并且接收到block的DataNode负责向其他DataNode复制block的副本（pipeline方式）</em></p>\n<hr>\n<p><a href=\"images/hdfs-write.png\">HDFS写数据流程图</a></p>\n<hr>\n<h5 id=\"写数据详解\"><a href=\"#写数据详解\" class=\"headerlink\" title=\"写数据详解\"></a>写数据详解</h5><ol>\n<li>Clinet向NameNode通信请求上传文件（hadoop fs -put test.log /test），NameNode检查目标文件是否存在，父目录是否存在,<br>调用分布式文件系统 DistributedFileSystem.create( )方法；</li>\n<li>NameNode 创建文件记录：分布式文件系统 DistributedFileSystem 发送 RPC 请求给 NameNode，NameNode 检查权限后创建一条记录，<br>返回输出流 FSDataOutputStream，封装了输出流 DFSOutputDtream；</li>\n<li>客户端写入数据：输出流 DFSOutputDtream 将数据分成一个个的数据包，并写入内部队列。DataStreamer 根据 DataNode 列表来要求 NameNode 分配适合的新块来存储数据备份。<br>一组 DataNode 构成管线(管线的 DataNode 之间使用 Socket 流式通信)；</li>\n<li>使用管线传输数据：DataStreamer 将数据包流式传输到管线第一个DataNode，第一个 DataNode 再传到第二个DataNode，直到完成；</li>\n<li>确认队列：DataNode 收到数据后发送确认，管线的 DataNode 所有的确认组成一个确认队列。所有 DataNode 都确认，管线数据包删除；</li>\n<li>关闭：客户端对数据量调用 close( ) 方法。将剩余所有数据写入DataNode管线，联系NameNode并且发送文件写入完成信息之前等待确认；</li>\n<li>NameNode确认：</li>\n<li>故障处理：若过程中发生故障，则先关闭管线，把队列中所有数据包添加回去队列，确保数据包不漏。为另一个正常 DataNode 的当前数据块指定一个新的标识，<br>并将该标识传送给 NameNode，一遍故障 DataNode 在恢复后删除上面的不完整数据块。从管线中删除故障 DataNode 并把余下的数据块写入余下正常的 DataNode。<br>NameNode 发现复本两不足时，会在另一个节点创建一个新的复本；</li>\n</ol>\n<p><strong>在数据的读取过程中难免碰到网络故障，脏数据，DataNode 失效等问题，这些问题 HDFS 在设计的时候都早已考虑到了。下面来介绍一下数据损坏处理流程：</strong></p>\n<ul>\n<li>当 DataNode 读取 block 的时候，它会计算 checksum。</li>\n<li>如果计算后的 checksum，与 block 创建时值不一样，说明该 block 已经损坏。</li>\n<li>Client 读取其它 DataNode上的 block。</li>\n<li>NameNode 标记该块已经损坏，然后复制 block 达到预期设置的文件备份数 。</li>\n<li>DataNode 在其文件创建后验证其 checksum。</li>\n</ul>\n<h4 id=\"HDFS读数据流程\"><a href=\"#HDFS读数据流程\" class=\"headerlink\" title=\"HDFS读数据流程\"></a>HDFS读数据流程</h4><h6 id=\"简述-1\"><a href=\"#简述-1\" class=\"headerlink\" title=\"简述\"></a>简述</h6><p><em>客户端将要读取的文件路径发送给namenode，namenode获取文件的元信息（主要是block的存放位置信息）返回给客户端，<br>客户端根据返回的信息找到相应datanode逐个获取文件的block并在客户端本地进行数据追加合并从而获得整个文件</em></p>\n<hr>\n<p><a href=\"images/hdfs-read.png\">HDFS写数据流程图</a></p>\n<hr>\n<ol>\n<li>打开分布式文件：调用分布式文件 DistributedFileSystem.open() 方法；</li>\n<li>寻址请求：从 NameNode 处得到 DataNode 的地址，DistributedFileSystem使用 RPC 方式调用了NameNode，NameNode 返回存有该副本的DataNode 地址，<br>DistributedFileSystem 返回了一个输入流对象（FSDataInputStream），该对象封装了输入流 DFSInputStream；</li>\n<li>连接到DataNode：调用输入流 FSDataInputStream.read() 方法从而让DFSInputStream 连接到 DataNodes；</li>\n<li>从 DataNode 中获取数据：通过循环调用 read() 方法，从而将数据从 DataNode 传输到客户端；</li>\n<li>读取另外的 DataNode 直到完成：到达块的末端时候，输入流 DFSInputStream 关闭与 DataNode 连接，寻找下一个 DataNode；</li>\n<li>完成读取，关闭连接：即调用输入流 FSDataInputStream.close()；</li>\n</ol>\n<h4 id=\"NAMENODE工作机制\"><a href=\"#NAMENODE工作机制\" class=\"headerlink\" title=\"NAMENODE工作机制\"></a>NAMENODE工作机制</h4><h6 id=\"职责\"><a href=\"#职责\" class=\"headerlink\" title=\"职责\"></a>职责</h6><ul>\n<li><p>负责客户端请求的响应</p>\n</li>\n<li><p>元数据的管理（查询、修改）[三种存储形式]</p>\n<ol>\n<li>内存元数据</li>\n<li>磁盘元数据镜像文件</li>\n<li>数据操作日志文件（通过日志算出元数据）</li>\n</ol>\n</li>\n<li><p>元数据的存储机制</p>\n<ol>\n<li>内存中有一份完整的元数据（metadata）</li>\n<li>磁盘有一个”准完整”的元数据镜像（fsimage）文件（在NameNode的工作目录中）</li>\n<li>用于衔接内存metadata和持久化元数据镜像fsimage之间的操作日志（edits文件）</li>\n</ol>\n</li>\n<li><p>元数据的手动查看</p>\n</li>\n</ul>\n<p><code>hdfs oev -i edits -o edits.xml</code></p>\n<p><code>hdfs oiv -i fsimage_0000000000000000087 -p XML -o fsimage.xml</code></p>\n<ul>\n<li>元数据的checkpoint</li>\n</ul>\n<p><em>每隔一段时间，会有secondary namenode 将NameNode上积累的所有edits和一个最新的fsimage下载到本地，并加载到内存进行merge</em></p>\n<hr>\n<p><a href=\"images/checkpoint.png\">元数据的checkpoint流程图</a></p>\n<hr>\n<ul>\n<li><p>checkpoint操作的触发条件配置参数</p>\n</li>\n<li><p>dfs.namenode.checkpoint.check.period=60  #检查触发条件是否满足的频率，60秒</p>\n</li>\n<li><p>dfs.namenode.checkpoint.dir=file://${hadoop.tmp.dir}/dfs/namesecondary</p>\n</li>\n<li><p>dfs.namenode.checkpoint.edits.dir=${dfs.namenode.checkpoint.dir} #以上两个参数做checkpoint操作时，secondary namenode的本地工作目录</p>\n</li>\n<li><p>dfs.namenode.checkpoint.max-retries=3  #最大重试次数</p>\n</li>\n<li><p>dfs.namenode.checkpoint.period=3600  #两次checkpoint之间的时间间隔3600秒</p>\n</li>\n<li><p>dfs.namenode.checkpoint.txns=1000000 #两次checkpoint之间最大的操作记录</p>\n</li>\n<li><p>checkpoint的附带作用:namenode和secondary namenode的工作目录存储结构完全相同，所以，当namenode故障退出需要重新恢复时，<br>可以从secondary namenode的工作目录中将fsimage拷贝到namenode的工作目录，以恢复namenode的元数据</p>\n</li>\n</ul>\n<h4 id=\"NAMENODE工作机制-1\"><a href=\"#NAMENODE工作机制-1\" class=\"headerlink\" title=\"NAMENODE工作机制\"></a>NAMENODE工作机制</h4><h6 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h6><ul>\n<li><p>DataNode工作职责：</p>\n<ol>\n<li>存储管理用户的文件块数据（实际数据）</li>\n<li>定期向NameNode汇报自身所持有的block信息（心跳机制）</li>\n</ol>\n</li>\n<li><p>Datanode掉线判断时限参数</p>\n</li>\n<li><p>datanode进程死亡或者网络故障造成datanode无法与namenode通信，namenode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。<br>HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为:</p>\n<br>\n`timeout = 2 * heartbeat.recheck.interval + 10 * dfs.heartbeat.interval`\n<br>\n而默认的heartbeat.recheck.interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。\n需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。\n所以，举个例子，如果heartbeat.recheck.interval设置为5000（毫秒），dfs.heartbeat.interval设置为3（秒，默认），则总的超时时间为40秒。\n\n\n</li>\n</ul>\n<h4 id=\"SecondaryNameNode\"><a href=\"#SecondaryNameNode\" class=\"headerlink\" title=\"SecondaryNameNode\"></a>SecondaryNameNode</h4><ul>\n<li><p>hadoop2.x以前存在单点故障，之后废弃了secondarynamenode，引入HA</p>\n</li>\n<li><p>HA有2中方式：NFS（Network File System）、QJL（Quorum Journal Node）(推荐)</p>\n</li>\n<li><p>以zookeeper做协调工具，部署2n+1台qjl实现高可用，qjl负责周期读取editslog文件并将fsimage与之合并形成新的fsimage（只要过半节点写入journalnode上即成功）</p>\n</li>\n<li><p>参考 <a href=\"https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/index.html\" target=\"_blank\" rel=\"noopener\">High Availability</a></p>\n</li>\n</ul>\n","categories":["大数据"],"tags":["大数据"]}]